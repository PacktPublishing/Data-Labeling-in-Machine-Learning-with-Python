{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.7102 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7066 - accuracy: 0.2500\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6959 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6786 - accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "raw_prediction: [[0.50196236]]\n",
      "prediction: [[1]]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I love this movie\", \"This movie is terrible\", \"The acting was amazing\", \"The plot was confusing\"] \n",
    "\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative \n",
    "\n",
    "# We then use a tokenizer to convert the text into sequences of numbers, and then pad the sequences so that they have the same length. We then define a generative AI model with an embedding layer, flatten layer, and dense layer. then compile and train the model on the training data. Finally, then we use the trained model to classify a new sentence as either positive or negative. \n",
    "\n",
    "# Here is a  complete python code example with a dataset of four sentences labelled as positive or negative. Importing Libraries: \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow import keras \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "#The numpy library is imported as np for numerical computations. The necessary modules from the TensorFlow library are imported for text preprocessing and model creation. Define the Labeled Dataset: \n",
    "\n",
    "sentences = [\"I love this movie\", \"This movie is terrible\", \"The acting was amazing\", \"The plot was confusing\"] \n",
    "\n",
    "labels = [1, 0, 1, 0] \n",
    "\n",
    "#The sentences list contains textual sentences. The labels list contains corresponding labels where 1 represents a positive sentiment and 0 represents a negative sentiment. Tokenize the Text and Convert to Sequences: \n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences) \n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences) \n",
    "\n",
    "#A Tokenizer object is created to tokenize the text. The fit_on_texts method is used to fit the tokenizer on the provided sentences. The texts_to_sequences method is used to convert the sentences into sequences of tokens. Pad the Sequences to Have the Same Length: \n",
    "\n",
    "max_sequence_length = max([len(seq) for seq in sequences]) \n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length) \n",
    "\n",
    " \n",
    "\n",
    "#The maximum sequence length is determined by finding the length of the longest sequence. The pad_sequences function is used to pad the sequences to the maximum length. Define the Model Architecture: \n",
    "\n",
    "model = keras.Sequential([ \n",
    "\n",
    "    keras.layers.Embedding(len(tokenizer.word_index) + 1, 16, input_length=max_sequence_length), \n",
    "\n",
    "    keras.layers.Flatten(), \n",
    "\n",
    "    keras.layers.Dense(1, activation='sigmoid') \n",
    "\n",
    "]) \n",
    "\n",
    "#A sequential model is created using the Sequential class from Keras. The model consists of an embedding layer, a flatten layer, and a dense layer. The embedding layer converts the tokens into dense vectors. The flatten layer flattens the input for the subsequent dense layer. The dense layer is used for binary classification with sigmoid activation. Compile the Model: \n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#The model is compiled with the Adam optimizer, binary cross-entropy loss, and accuracy as the metric. Train the Model: \n",
    "\n",
    "model.fit(padded_sequences, np.array(labels), epochs=10) \n",
    "\n",
    "#The model is trained on the padded sequences and corresponding labels for a specified number of epochs. Classify a New Sentence: \n",
    "\n",
    "new_sentence = [\"This movie is good\"] \n",
    "\n",
    "new_sequence = tokenizer.texts_to_sequences(new_sentence) \n",
    "\n",
    "padded_new_sequence = pad_sequences(new_sequence, maxlen=max_sequence_length) \n",
    "\n",
    "raw_prediction = model.predict(padded_new_sequence)\n",
    "print(\"raw_prediction:\",raw_prediction)\n",
    "prediction = (raw_prediction > 0.5).astype('int32')\n",
    "\n",
    "print(\"prediction:\",prediction)\n",
    "#A new sentence is provided for classification. The sentence is converted to a sequence of tokens using the tokenizer. The sequence is padded to match the maximum sequence length used during training. The model predicts the sentiment class for the new sentence. \n",
    "\n",
    "#Print the Predicted Label: \n",
    "\n",
    "if prediction[0][0] == 1: \n",
    "\n",
    "    print(\"Positive\") \n",
    "\n",
    "else: \n",
    "\n",
    "    print(\"Negative\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
