{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytube in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (15.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download the audio stream from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are importing Pytube library\n",
    "import pytube\n",
    "#we are downloading youtube video from Youtube link\n",
    "video = \"https://youtu.be/g8Q452PEXwY\"\n",
    "data = pytube.YouTube(video)\n",
    "# Converting and downloading as 'MP4' file\n",
    "audio = data.streams.get_audio_only()\n",
    "audio.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai-whisper in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (20231117)\n",
      "Requirement already satisfied: numba in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (0.58.1)\n",
      "Requirement already satisfied: numpy in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (1.24.3)\n",
      "Requirement already satisfied: torch in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (4.64.1)\n",
      "Requirement already satisfied: more-itertools in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (10.1.0)\n",
      "Requirement already satisfied: tiktoken in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from openai-whisper) (0.5.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from numba->openai-whisper) (0.41.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from tiktoken->openai-whisper) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: filelock in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from torch->openai-whisper) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from jinja2->torch->openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install openai whisper\n",
    "%pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import whisper\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the FFMPEG environment variable to the path of your ffmpeg executable\n",
    "os.environ['ffmpeg'] = '/Users/<username>/audio-orchestrator-ffmpeg/bin/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '/Users/<username>/audio-orchestrator-ffmpeg/bin:' + os.environ['PATH']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Transcribe a audio 'Mel Spectrograms with Python and Librosa  Audio Feature Extraction.mp4' that we downloaded in previous cell from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudachk/Library/Python/3.9/lib/python/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello, in this video we'll go over an important audio feature to extract, called Mel Spectrums, and how we can extract Mel Spectrums using Python and Librosa. Let's get started. Firstly, audio feature extraction is a crucial step in machine learning, converting raw audio data into a structured format suitable for model training and analysis. Raw audio data is complex and contains vast amount of information. Each extraction reduces the dimensionality, emphasizing relevant patterns for machine learning models. The Mel Spectrum, a visual representation of frequency content over time, is a powerful feature. And in this video we'll understand how we can leverage a Mel scale to approximate the human ears response to different frequencies, making it particularly valuable for tasks involving human perception. A Mel Spectrum transforms the linear frequency scale of a traditional spectrogram into a logarithmic scale. Now let's implement audio feature extraction using Mel Spectrums and Python and Librosa. Firstly, let's import all the necessary libraries, including Librosa for audio processing, Mod Plotlib for the plotting, and NumPy for numerical operations, will also import ipython.display to be able to hear our audio file. Next, let's store the audio file path into a variable, and then let's hear our audio file. As you may know, that is the F major scale. And now let's get the Y, which is the audio time series data, and also the SR, which is the sampling rate of the time series data. So we can do that using a simple function in Librosa, called Librosa.load, given an audio file path. And there's other arguments we can pass in, like the NFFTs, number of samples, in each of the short-forare transforms it does, and normally it defaults to 2048, so that's good in our case. And we can also change the hotplanned number of males, and even the frequency min and maximum. We'll keep those for the default from now. And after this, let's actually extract the Mel Spectrum. Librosa makes this easy through a function called Librosa.feature.mel Spectrum. And here we just input the signals we have. So we'll take the Y that we got from Librosa.load, and also the sampling rate from Librosa.load. And now let's run this. Next we'll convert the linear scale spectrogram to decibels, and all we have to do is input our Mel Spectrum that we have extracted, and provide a reference power to use when we're converting to decimals. And in this case, we'll use numpy.max. After this, all we have to do is plot the Mel Spectrum using Librosa.display.specshow, and we'll also use mapplot.pyplot. So there's other things we can do, including adjusting the figure size, the axis, the color map, etc. And let's run this again. And as we can see, this is the Mel Spectrum again. So this is a visual representation of the audio having the frequency over time. As you can see, it's a pretty simple and quick process to extract a Mel Spectrum. So it can be a very powerful tool when training a machine learning model on audio data. Hope that was helpful, and see you in the next video.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = whisper.load_model('base')\n",
    "text = model.transcribe('Mel Spectrograms with Python and Librosa  Audio Feature Extraction.mp4')\n",
    "#printing the transcribe\n",
    "text['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example 2: Transcribe a audio \"customer_call_audio.m4a\" using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudachk/Library/Python/3.9/lib/python/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hello, I have not received the product yet. I am very disappointed. Are you going to replace if my product is damaged or missed? I will be happy if you replace with your product in case I miss the product due to incorrect shipping address.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = whisper.load_model('base')\n",
    "text = model.transcribe('../Ch11/customer_call_audio.m4a')\n",
    "#printing the transcribe\n",
    "text['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Hello, in this video we'll go over an important audio feature to extract, called Mel Spectrums, and how we can extract Mel Spectrums using Python and Librosa. Let's get started. Firstly, audio feature extraction is a crucial step in machine learning, converting raw audio data into a structured format suitable for model training and analysis. Raw audio data is complex and contains vast amount of information. Each extraction reduces the dimensionality, emphasizing relevant patterns for machine learning models. The Mel Spectrum, a visual representation of frequency content over time, is a powerful feature. And in this video we'll understand how we can leverage a Mel scale to approximate the human ears response to different frequencies, making it particularly valuable for tasks involving human perception. A Mel Spectrum transforms the linear frequency scale of a traditional spectrogram into a logarithmic scale. Now let's implement audio feature extraction using Mel Spectrums and Python and Librosa. Firstly, let's import all the necessary libraries, including Librosa for audio processing, Mod Plotlib for the plotting, and NumPy for numerical operations, will also import ipython.display to be able to hear our audio file. Next, let's store the audio file path into a variable, and then let's hear our audio file. As you may know, that is the F major scale. And now let's get the Y, which is the audio time series data, and also the SR, which is the sampling rate of the time series data. So we can do that using a simple function in Librosa, called Librosa.load, given an audio file path. And there's other arguments we can pass in, like the NFFTs, number of samples, in each of the short-forare transforms it does, and normally it defaults to 2048, so that's good in our case. And we can also change the hotplanned number of males, and even the frequency min and maximum. We'll keep those for the default from now. And after this, let's actually extract the Mel Spectrum. Librosa makes this easy through a function called Librosa.feature.mel Spectrum. And here we just input the signals we have. So we'll take the Y that we got from Librosa.load, and also the sampling rate from Librosa.load. And now let's run this. Next we'll convert the linear scale spectrogram to decibels, and all we have to do is input our Mel Spectrum that we have extracted, and provide a reference power to use when we're converting to decimals. And in this case, we'll use numpy.max. After this, all we have to do is plot the Mel Spectrum using Librosa.display.specshow, and we'll also use mapplot.pyplot. So there's other things we can do, including adjusting the figure size, the axis, the color map, etc. And let's run this again. And as we can see, this is the Mel Spectrum again. So this is a visual representation of the audio having the frequency over time. As you can see, it's a pretty simple and quick process to extract a Mel Spectrum. So it can be a very powerful tool when training a machine learning model on audio data. Hope that was helpful, and see you in the next video.\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 6.44, 'text': \" Hello, in this video we'll go over an important audio feature to extract, called Mel Spectrums,\", 'tokens': [50364, 2425, 11, 294, 341, 960, 321, 603, 352, 670, 364, 1021, 6278, 4111, 281, 8947, 11, 1219, 7375, 27078, 6247, 82, 11, 50686], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 1, 'seek': 0, 'start': 6.44, 'end': 10.96, 'text': ' and how we can extract Mel Spectrums using Python and Librosa.', 'tokens': [50686, 293, 577, 321, 393, 8947, 7375, 27078, 6247, 82, 1228, 15329, 293, 15834, 2635, 64, 13, 50912], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 2, 'seek': 0, 'start': 10.96, 'end': 12.120000000000001, 'text': \" Let's get started.\", 'tokens': [50912, 961, 311, 483, 1409, 13, 50970], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 3, 'seek': 0, 'start': 12.120000000000001, 'end': 17.2, 'text': ' Firstly, audio feature extraction is a crucial step in machine learning, converting raw audio', 'tokens': [50970, 20042, 11, 6278, 4111, 30197, 307, 257, 11462, 1823, 294, 3479, 2539, 11, 29942, 8936, 6278, 51224], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 4, 'seek': 0, 'start': 17.2, 'end': 22.400000000000002, 'text': ' data into a structured format suitable for model training and analysis.', 'tokens': [51224, 1412, 666, 257, 18519, 7877, 12873, 337, 2316, 3097, 293, 5215, 13, 51484], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 5, 'seek': 0, 'start': 22.400000000000002, 'end': 26.32, 'text': ' Raw audio data is complex and contains vast amount of information.', 'tokens': [51484, 23732, 6278, 1412, 307, 3997, 293, 8306, 8369, 2372, 295, 1589, 13, 51680], 'temperature': 0.0, 'avg_logprob': -0.2506288351471891, 'compression_ratio': 1.6599190283400809, 'no_speech_prob': 0.0322660468518734}, {'id': 6, 'seek': 2632, 'start': 26.32, 'end': 31.36, 'text': ' Each extraction reduces the dimensionality, emphasizing relevant patterns for machine learning', 'tokens': [50364, 6947, 30197, 18081, 264, 10139, 1860, 11, 45550, 7340, 8294, 337, 3479, 2539, 50616], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 7, 'seek': 2632, 'start': 31.36, 'end': 32.36, 'text': ' models.', 'tokens': [50616, 5245, 13, 50666], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 8, 'seek': 2632, 'start': 32.36, 'end': 36.94, 'text': ' The Mel Spectrum, a visual representation of frequency content over time, is a powerful', 'tokens': [50666, 440, 7375, 27078, 6247, 11, 257, 5056, 10290, 295, 7893, 2701, 670, 565, 11, 307, 257, 4005, 50895], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 9, 'seek': 2632, 'start': 36.94, 'end': 37.94, 'text': ' feature.', 'tokens': [50895, 4111, 13, 50945], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 10, 'seek': 2632, 'start': 37.94, 'end': 42.56, 'text': \" And in this video we'll understand how we can leverage a Mel scale to approximate the\", 'tokens': [50945, 400, 294, 341, 960, 321, 603, 1223, 577, 321, 393, 13982, 257, 7375, 4373, 281, 30874, 264, 51176], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 11, 'seek': 2632, 'start': 42.56, 'end': 48.24, 'text': ' human ears response to different frequencies, making it particularly valuable for tasks involving', 'tokens': [51176, 1952, 8798, 4134, 281, 819, 20250, 11, 1455, 309, 4098, 8263, 337, 9608, 17030, 51460], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 12, 'seek': 2632, 'start': 48.24, 'end': 49.24, 'text': ' human perception.', 'tokens': [51460, 1952, 12860, 13, 51510], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 13, 'seek': 2632, 'start': 49.24, 'end': 54.64, 'text': ' A Mel Spectrum transforms the linear frequency scale of a traditional spectrogram into a', 'tokens': [51510, 316, 7375, 27078, 6247, 35592, 264, 8213, 7893, 4373, 295, 257, 5164, 6177, 6675, 2356, 666, 257, 51780], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 14, 'seek': 2632, 'start': 54.64, 'end': 56.28, 'text': ' logarithmic scale.', 'tokens': [51780, 41473, 355, 13195, 4373, 13, 51862], 'temperature': 0.0, 'avg_logprob': -0.24744437824596058, 'compression_ratio': 1.6419354838709677, 'no_speech_prob': 0.016780385747551918}, {'id': 15, 'seek': 5628, 'start': 56.28, 'end': 63.480000000000004, 'text': \" Now let's implement audio feature extraction using Mel Spectrums and Python and Librosa.\", 'tokens': [50364, 823, 718, 311, 4445, 6278, 4111, 30197, 1228, 7375, 27078, 6247, 82, 293, 15329, 293, 15834, 2635, 64, 13, 50724], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 16, 'seek': 5628, 'start': 63.480000000000004, 'end': 69.24000000000001, 'text': \" Firstly, let's import all the necessary libraries, including Librosa for audio processing,\", 'tokens': [50724, 20042, 11, 718, 311, 974, 439, 264, 4818, 15148, 11, 3009, 15834, 2635, 64, 337, 6278, 9007, 11, 51012], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 17, 'seek': 5628, 'start': 69.24000000000001, 'end': 75.02000000000001, 'text': ' Mod Plotlib for the plotting, and NumPy for numerical operations, will also import', 'tokens': [51012, 6583, 2149, 310, 38270, 337, 264, 41178, 11, 293, 22592, 47, 88, 337, 29054, 7705, 11, 486, 611, 974, 51301], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 18, 'seek': 5628, 'start': 75.02000000000001, 'end': 79.56, 'text': ' ipython.display to be able to hear our audio file.', 'tokens': [51301, 28501, 88, 11943, 13, 35238, 281, 312, 1075, 281, 1568, 527, 6278, 3991, 13, 51528], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 19, 'seek': 5628, 'start': 79.56, 'end': 84.56, 'text': \" Next, let's store the audio file path into a variable, and then let's hear our audio\", 'tokens': [51528, 3087, 11, 718, 311, 3531, 264, 6278, 3991, 3100, 666, 257, 7006, 11, 293, 550, 718, 311, 1568, 527, 6278, 51778], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 20, 'seek': 5628, 'start': 84.56, 'end': 85.56, 'text': ' file.', 'tokens': [51778, 3991, 13, 51828], 'temperature': 0.0, 'avg_logprob': -0.2604439933345003, 'compression_ratio': 1.7264957264957266, 'no_speech_prob': 0.0004091981681995094}, {'id': 21, 'seek': 8556, 'start': 85.88, 'end': 94.60000000000001, 'text': ' As you may know, that is the F major scale.', 'tokens': [50380, 1018, 291, 815, 458, 11, 300, 307, 264, 479, 2563, 4373, 13, 50816], 'temperature': 0.0, 'avg_logprob': -0.2317988843802946, 'compression_ratio': 1.5895953757225434, 'no_speech_prob': 0.0069504124112427235}, {'id': 22, 'seek': 8556, 'start': 94.60000000000001, 'end': 100.64, 'text': \" And now let's get the Y, which is the audio time series data, and also the SR, which is\", 'tokens': [50816, 400, 586, 718, 311, 483, 264, 398, 11, 597, 307, 264, 6278, 565, 2638, 1412, 11, 293, 611, 264, 20840, 11, 597, 307, 51118], 'temperature': 0.0, 'avg_logprob': -0.2317988843802946, 'compression_ratio': 1.5895953757225434, 'no_speech_prob': 0.0069504124112427235}, {'id': 23, 'seek': 8556, 'start': 100.64, 'end': 103.84, 'text': ' the sampling rate of the time series data.', 'tokens': [51118, 264, 21179, 3314, 295, 264, 565, 2638, 1412, 13, 51278], 'temperature': 0.0, 'avg_logprob': -0.2317988843802946, 'compression_ratio': 1.5895953757225434, 'no_speech_prob': 0.0069504124112427235}, {'id': 24, 'seek': 8556, 'start': 103.84, 'end': 109.32000000000001, 'text': ' So we can do that using a simple function in Librosa, called Librosa.load, given an', 'tokens': [51278, 407, 321, 393, 360, 300, 1228, 257, 2199, 2445, 294, 15834, 2635, 64, 11, 1219, 15834, 2635, 64, 13, 2907, 11, 2212, 364, 51552], 'temperature': 0.0, 'avg_logprob': -0.2317988843802946, 'compression_ratio': 1.5895953757225434, 'no_speech_prob': 0.0069504124112427235}, {'id': 25, 'seek': 8556, 'start': 109.32000000000001, 'end': 111.4, 'text': ' audio file path.', 'tokens': [51552, 6278, 3991, 3100, 13, 51656], 'temperature': 0.0, 'avg_logprob': -0.2317988843802946, 'compression_ratio': 1.5895953757225434, 'no_speech_prob': 0.0069504124112427235}, {'id': 26, 'seek': 11140, 'start': 111.4, 'end': 117.0, 'text': \" And there's other arguments we can pass in, like the NFFTs, number of samples, in each\", 'tokens': [50364, 400, 456, 311, 661, 12869, 321, 393, 1320, 294, 11, 411, 264, 13576, 25469, 82, 11, 1230, 295, 10938, 11, 294, 1184, 50644], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 27, 'seek': 11140, 'start': 117.0, 'end': 122.12, 'text': \" of the short-forare transforms it does, and normally it defaults to 2048, so that's\", 'tokens': [50644, 295, 264, 2099, 12, 2994, 543, 35592, 309, 775, 11, 293, 5646, 309, 7576, 82, 281, 945, 13318, 11, 370, 300, 311, 50900], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 28, 'seek': 11140, 'start': 122.12, 'end': 123.72, 'text': ' good in our case.', 'tokens': [50900, 665, 294, 527, 1389, 13, 50980], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 29, 'seek': 11140, 'start': 123.72, 'end': 129.28, 'text': ' And we can also change the hotplanned number of males, and even the frequency min and maximum.', 'tokens': [50980, 400, 321, 393, 611, 1319, 264, 2368, 564, 5943, 1230, 295, 20776, 11, 293, 754, 264, 7893, 923, 293, 6674, 13, 51258], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 30, 'seek': 11140, 'start': 129.28, 'end': 132.72, 'text': \" We'll keep those for the default from now.\", 'tokens': [51258, 492, 603, 1066, 729, 337, 264, 7576, 490, 586, 13, 51430], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 31, 'seek': 11140, 'start': 132.72, 'end': 135.84, 'text': \" And after this, let's actually extract the Mel Spectrum.\", 'tokens': [51430, 400, 934, 341, 11, 718, 311, 767, 8947, 264, 7375, 27078, 6247, 13, 51586], 'temperature': 0.0, 'avg_logprob': -0.31798753114504236, 'compression_ratio': 1.563265306122449, 'no_speech_prob': 0.0173511765897274}, {'id': 32, 'seek': 13584, 'start': 135.84, 'end': 141.6, 'text': ' Librosa makes this easy through a function called Librosa.feature.mel Spectrum.', 'tokens': [50364, 15834, 2635, 64, 1669, 341, 1858, 807, 257, 2445, 1219, 15834, 2635, 64, 13, 2106, 1503, 13, 10909, 27078, 6247, 13, 50652], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 33, 'seek': 13584, 'start': 141.6, 'end': 143.48000000000002, 'text': ' And here we just input the signals we have.', 'tokens': [50652, 400, 510, 321, 445, 4846, 264, 12354, 321, 362, 13, 50746], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 34, 'seek': 13584, 'start': 143.48000000000002, 'end': 151.04, 'text': \" So we'll take the Y that we got from Librosa.load, and also the sampling rate from Librosa.load.\", 'tokens': [50746, 407, 321, 603, 747, 264, 398, 300, 321, 658, 490, 15834, 2635, 64, 13, 2907, 11, 293, 611, 264, 21179, 3314, 490, 15834, 2635, 64, 13, 2907, 13, 51124], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 35, 'seek': 13584, 'start': 151.04, 'end': 152.6, 'text': \" And now let's run this.\", 'tokens': [51124, 400, 586, 718, 311, 1190, 341, 13, 51202], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 36, 'seek': 13584, 'start': 152.6, 'end': 158.0, 'text': \" Next we'll convert the linear scale spectrogram to decibels, and all we have to do is input\", 'tokens': [51202, 3087, 321, 603, 7620, 264, 8213, 4373, 6177, 340, 1342, 281, 46358, 38383, 11, 293, 439, 321, 362, 281, 360, 307, 4846, 51472], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 37, 'seek': 13584, 'start': 158.0, 'end': 163.48000000000002, 'text': \" our Mel Spectrum that we have extracted, and provide a reference power to use when we're\", 'tokens': [51472, 527, 7375, 27078, 6247, 300, 321, 362, 34086, 11, 293, 2893, 257, 6408, 1347, 281, 764, 562, 321, 434, 51746], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 38, 'seek': 13584, 'start': 163.48000000000002, 'end': 165.24, 'text': ' converting to decimals.', 'tokens': [51746, 29942, 281, 979, 332, 1124, 13, 51834], 'temperature': 0.0, 'avg_logprob': -0.21167020280231802, 'compression_ratio': 1.7335907335907337, 'no_speech_prob': 0.0974128395318985}, {'id': 39, 'seek': 16524, 'start': 165.28, 'end': 167.8, 'text': \" And in this case, we'll use numpy.max.\", 'tokens': [50366, 400, 294, 341, 1389, 11, 321, 603, 764, 1031, 8200, 13, 41167, 13, 50492], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 40, 'seek': 16524, 'start': 167.8, 'end': 173.4, 'text': ' After this, all we have to do is plot the Mel Spectrum using Librosa.display.specshow,', 'tokens': [50492, 2381, 341, 11, 439, 321, 362, 281, 360, 307, 7542, 264, 7375, 27078, 6247, 1228, 15834, 2635, 64, 13, 35238, 13, 7053, 66, 34436, 11, 50772], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 41, 'seek': 16524, 'start': 173.4, 'end': 176.36, 'text': \" and we'll also use mapplot.pyplot.\", 'tokens': [50772, 293, 321, 603, 611, 764, 4471, 564, 310, 13, 8200, 564, 310, 13, 50920], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 42, 'seek': 16524, 'start': 176.36, 'end': 181.08, 'text': \" So there's other things we can do, including adjusting the figure size, the axis, the\", 'tokens': [50920, 407, 456, 311, 661, 721, 321, 393, 360, 11, 3009, 23559, 264, 2573, 2744, 11, 264, 10298, 11, 264, 51156], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 43, 'seek': 16524, 'start': 181.08, 'end': 183.60000000000002, 'text': ' color map, etc.', 'tokens': [51156, 2017, 4471, 11, 5183, 13, 51282], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 44, 'seek': 16524, 'start': 183.60000000000002, 'end': 186.28, 'text': \" And let's run this again.\", 'tokens': [51282, 400, 718, 311, 1190, 341, 797, 13, 51416], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 45, 'seek': 16524, 'start': 186.28, 'end': 188.28, 'text': ' And as we can see, this is the Mel Spectrum again.', 'tokens': [51416, 400, 382, 321, 393, 536, 11, 341, 307, 264, 7375, 27078, 6247, 797, 13, 51516], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 46, 'seek': 16524, 'start': 188.28, 'end': 192.84, 'text': ' So this is a visual representation of the audio having the frequency over time.', 'tokens': [51516, 407, 341, 307, 257, 5056, 10290, 295, 264, 6278, 1419, 264, 7893, 670, 565, 13, 51744], 'temperature': 0.0, 'avg_logprob': -0.19677159213280493, 'compression_ratio': 1.676, 'no_speech_prob': 0.0013748734490945935}, {'id': 47, 'seek': 19284, 'start': 192.84, 'end': 197.20000000000002, 'text': \" As you can see, it's a pretty simple and quick process to extract a Mel Spectrum.\", 'tokens': [50364, 1018, 291, 393, 536, 11, 309, 311, 257, 1238, 2199, 293, 1702, 1399, 281, 8947, 257, 7375, 27078, 6247, 13, 50582], 'temperature': 0.0, 'avg_logprob': -0.1912043341274919, 'compression_ratio': 1.3680981595092025, 'no_speech_prob': 0.040090713649988174}, {'id': 48, 'seek': 19284, 'start': 197.20000000000002, 'end': 202.4, 'text': ' So it can be a very powerful tool when training a machine learning model on audio data.', 'tokens': [50582, 407, 309, 393, 312, 257, 588, 4005, 2290, 562, 3097, 257, 3479, 2539, 2316, 322, 6278, 1412, 13, 50842], 'temperature': 0.0, 'avg_logprob': -0.1912043341274919, 'compression_ratio': 1.3680981595092025, 'no_speech_prob': 0.040090713649988174}, {'id': 49, 'seek': 19284, 'start': 202.4, 'end': 204.4, 'text': ' Hope that was helpful, and see you in the next video.', 'tokens': [50842, 6483, 300, 390, 4961, 11, 293, 536, 291, 294, 264, 958, 960, 13, 50942], 'temperature': 0.0, 'avg_logprob': -0.1912043341274919, 'compression_ratio': 1.3680981595092025, 'no_speech_prob': 0.040090713649988174}], 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 842 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.3)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: requests in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-macosx_11_0_arm64.whl (425 kB)\n",
      "\u001b[K     |████████████████████████████████| 425 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.17)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sudachk/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.1)\n",
      "Installing collected packages: huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9992625117301941}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# Example text for sentiment analysis\n",
    "#text = \"I love using Hugging Face Transformers! It's amazing.\"\n",
    "text=\"Hello, I have not received the product yet. I am very disappointed.are you going to replace if my product is damaged or missed.I will be happy if you replace with new product incase i missed the product die to incorrect shipping address\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "result = sentiment_classifier(text)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
